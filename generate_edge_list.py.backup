#!/usr/bin/env python3

import os
import re
import argparse
import sys

# Suppress TensorFlow warnings before importing any ML libraries
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow logging
os.environ['CUDA_VISIBLE_DEVICES'] = ''  # Optional: disable CUDA if not needed
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # Disable oneDNN optimizations warnings

# Suppress other warnings
import warnings
warnings.filterwarnings('ignore')

from dotenv import load_dotenv
from gliner import GLiNER
from unstructured.partition.pdf import partition_pdf
from unstructured.cleaners.core import (clean, 
                                        group_broken_paragraphs, 
                                        clean_extra_whitespace)

from ollama import chat
from pydantic import BaseModel

from langchain_ollama.llms import OllamaLLM
from langchain_ollama import OllamaEmbeddings
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.vectorstores import InMemoryVectorStore
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.text_splitter import RecursiveCharacterTextSplitter

class EdgeList():
    """
    Workflow for generating an edge list from a PDF file.
    """

    def __init__(self, file: str, ner_threshold: float = 0.65, k: int = 10):
        self.file = file
        self.file_name = os.path.basename(file)
        self.k = k
        self.ner_threshold = ner_threshold

        # Initialize the GLiNER NER model
        self.model = GLiNER.from_pretrained("numind/NuNER_Zero-span")
        self.labels = ["Person", "Company", "Organization"]

        # Partition the PDF file into elements, extracting sentences and connections
        self.elements = None
        self.sentences = None
        self.connections = None
        self.edge_list = []

        # Initialize the LLM, embeddings, vectorstore
        self.llm = OllamaLLM(model="mistral", temperature=0)
        self.retriever = None
        self.rag_chain = None

    def fit(self):
        # Partition the PDF file into elements, extracting sentences and connections
        self.elements = partition_pdf(self.file)
        self.sentences = self.extract_sentences()

    def transform(self):
        self.connections = self.get_connections()
        self.retriever = self.create_retriever()
        self.rag_chain = self.create_rag_chain()
        self.edge_list = self.create_edge_list()
    
    def fit_transform(self):
        """
        Fit and transform the edge list.
        """
        self.fit()
        self.transform()
        return self.edge_list
    
    def save_edge_list(self, file_name: str):
        """
        Save the edge list to a TSV file.
        """
        with open(file_name, "w") as f:
            f.write("source\ttarget\n")  # Header row
            for source, target in self.edge_list:
                f.write(f"{source}\t{target}\n")
        print(f"Edge list saved to {file_name}")

    def create_retriever(self):
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=100,chunk_overlap=55)
        embeddings = OllamaEmbeddings(model="mxbai-embed-large")
        vectorstore = InMemoryVectorStore(embeddings)
        docs = []
        for i, s in enumerate(self.sentences):
            docs.append(Document(id=str(i), page_content=s, metadata={"source": self.file_name}))
        all_texts = text_splitter.split_documents(docs)
        vs = vectorstore.from_documents(documents=all_texts, embedding=embeddings)
        retriever = vs.as_retriever(search_kwargs={"k": self.k, "score_threshold": 0.95})
        return retriever
    
    def create_rag_chain(self):
        system_prompt = (
            "You are a precise name identification assistant. Your task is to identify the full name of a person from the given context.\n"
            "The entity type hint is: {entity_type}\n"
            "Rules:\n"
            "1. If the entity type is 'Person' and you find the person's full name (first + last), return it exactly as written\n"
            "2. If the entity type is 'Person' and the name is explicitly mentioned as a pseudonym, fake name, or alias, return 'Name (pseudonym)'\n"
            "3. If the entity type is 'Person' and you cannot find the full name, return 'Name (unknown)'\n"
            "4. If the entity type is 'Organization' or 'Company', return the name as-is without any additional labels\n"
            "5. If the name is mentioned but unclear, or if there are multiple possibilities, return 'Name (unclear)'\n"
            "6. Only use information explicitly stated in the context\n"
            "7. Be factual. Do not invent new names. If you don't know the answer, respond with 'Name (unknown)'\n"
            "8. Be concise - provide only the name and status. Do not include any additional commentary, explanations or formatting.\n\n"
            "Examples:\n"
            "- Input: 'Rachel' (Person) → Output: 'Rachel Blais' (if full name found)\n"
            "- Input: 'Kate' (Person) → Output: 'Kate (pseudonym)' (if mentioned as fake name)\n"
            "- Input: 'Vogue' (Organization) → Output: 'Vogue' (return as-is)\n\n"
            "Context: {context}"
        )
        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", system_prompt),
                ("human", "Name: {input}\nEntity Type: {entity_type}"),
            ]
        )
        qa_chain = create_stuff_documents_chain(self.llm, prompt)
        rag_chain = create_retrieval_chain(self.retriever, qa_chain)
        return rag_chain

    def clean_text(self, text) -> str:
        """
        Cleans the text by applying various cleaning functions.
        """
        text = clean(text)
        text = group_broken_paragraphs(text)
        text = clean_extra_whitespace(text)
        return text

    def extract_sentences(self) -> list:
        self.sentences = []
        for e in self.elements:
            if e.text:
                # Clean the text
                text = self.clean_text(e.text)
                self.sentences.append(text)
        return self.sentences
    
    def get_connections(self):
        self.connections = []
        for sent in self.sentences:
            c = []
            entities = self.model.predict_entities(sent, self.labels, threshold=self.ner_threshold)
            for entity in entities:
                c.append((entity["text"], entity["label"]))
            c = list(set(c))  # Remove duplicates
            if len(c) > 1:
                self.connections.append(c)
        return self.connections
    
    def _identify_person(self, name: str, entity_type: str) -> str:
        """
        Identify a person's full name using the RAG pipeline with structured output.
        """
        query = {
            "input":  name,
            "entity_type": entity_type
        }
        response = self.rag_chain.invoke(query)
        answer = response['answer'].strip()
        return self._format_name_response(name, entity_type, answer)
    
    def _format_name_response(self, input_name: str, entity_type: str, response: str) -> str:
        """
        Format the response to ensure consistent output format.
        """
        response_lower = response.lower()
        if any(indicator in response_lower for indicator in ['pseudonym', 'fake name', 'alias', 'not real']):
            clean_name = response.split('(')[0].strip() if '(' in response else input_name
            return [clean_name, "(pseudonym)"]
        
        elif any(indicator in response_lower for indicator in ["unclear"]):
            return [input_name, "(unclear)"]

        elif any(indicator in response_lower for indicator in ["don't know", "unknown", "not found"]):
            return [input_name, "(unknown)"]

        elif len(response.split()) >= 2 and not any(word in response_lower for word in ['unknown', 'pseudonym']) or any(word in response_lower for word in ['real', 'real name']):
            pattern = r'\s*\(real(?:\s+name)?\)'
            cleaned = re.sub(pattern, '', response.strip())
            cleaned = cleaned.strip("'\"")
            return [cleaned, "(real name)"]

        else:
            return [input_name, f"({entity_type})"]
        
    def _check_name(self, name, entity_type):
        """ 
        Check if a name is a person using the RAG pipeline. 
        """
        class Person(BaseModel):
            name: str

        context = " ".join([r.page_content.strip(".").strip() for r in self.retriever.batch([name])[0]])

        response = chat(
        messages=[
            {
            'role': 'user',
            'content': f"""Please identify the full name of {name} if it is a person.
                           This {name} is of type {entity_type}. 
                           Here is some context: {context}. 
                           Provide the full name if available. 
                           If the name is not a person, return the name as is.
                           If the name is not a person (e.g., a company or organization), return the name as is without any additional labels.
                           Return the name without any additional formatting or quotes.""",
            }
        ],
        model='phi3.5', format=Person.model_json_schema(),
        )

        person = Person.model_validate_json(response.message.content)
        return person
    
    def _judge_name(self, name1, name2):
        """ 
        Check if a name is a person using the RAG pipeline. 
        """
        class Judgement(BaseModel):
            best: str

        response = chat(
        messages=[
            {
            'role': 'user',
            'content': f"""
                        You are given two names: {name1} and {name2}. 
                        You're goal is to judge the best representation of a person or organization's name.
                        Prefer full names over single names.
                        Only judge between the two names. Only return the best name.
                        """,
            }
        ],
        model='phi4', format=Judgement.model_json_schema(),
        )
        judgement = Judgement.model_validate_json(response.message.content)
        return judgement

    def create_edge_list(self):
        """
        Create an edge list from the connections.
        """
        edge_list = []
        for conn in self.connections:
            for i, c in enumerate(conn):
                if i == 0:
                    source, _ = self._final_name_check((c[0], c[1]))
                    continue
                target, _ = self._final_name_check((c[0], c[1]))
                if source == target:
                    continue
                edge_list.append((source, target))
        return list(set(edge_list))

    def _final_name_check(self, entity):
        """
        Given an entity tuple (name, entity_type), returns the best full name match.
        """
        name, entity_type = entity
        
        if entity_type in ['Organization', 'Company']:
            return (name, entity_type)
        
        try:
            res1 = self._identify_person(name, entity_type)
            if isinstance(res1, list) and len(res1) > 0:
                name1 = res1[0]
            else:
                name1 = str(res1) if res1 else name
        except Exception as e:
            print(f"Error in _identify_person: {e}")
            name1 = name
        
        try:
            res2 = self._check_name(name, entity_type)
            name2 = res2.name if hasattr(res2, 'name') else str(res2)
        except Exception as e:
            print(f"Error in _check_name: {e}")
            name2 = name
        
        def is_full_name(candidate_name):
            if not isinstance(candidate_name, str):
                return False
            
            clean_name = candidate_name.lower()
            status_indicators = ['(unknown)', '(pseudonym)', '(unclear)', '(person)', '(real name)']
            
            for indicator in status_indicators:
                if indicator in clean_name:
                    return False
            
            words = candidate_name.strip().split()
            return len(words) >= 2
        
        def clean_name(candidate_name):
            if not isinstance(candidate_name, str):
                return str(candidate_name)
            candidate_name = candidate_name.strip("'\"").strip()
            candidate_name = candidate_name.split('(')[0].strip()
            return candidate_name
        
        name1_clean = clean_name(name1)
        name2_clean = clean_name(name2)
        
        name1_is_full = is_full_name(name1_clean)
        name2_is_full = is_full_name(name2_clean)
        
        if name1_clean.lower() == name2_clean.lower():
            return (name1_clean, entity_type)
        
        if name1_is_full and name2_is_full:
            res = self._judge_name(name1_clean, name2_clean)
            return (res.best, entity_type)
        
        if name1_is_full and not name2_is_full:
            res = self._judge_name(name1_clean, name2_clean)
            return (res.best, entity_type)
        elif name2_is_full and not name1_is_full:
            res = self._judge_name(name1_clean, name2_clean)
            return (res.best, entity_type)
        
        return (name, entity_type)


def main():
    parser = argparse.ArgumentParser(
        description="Generate edge list from PDF file using Named Entity Recognition",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  %(prog)s document.pdf
  %(prog)s document.pdf -o edges.tsv
  %(prog)s document.pdf --ner-threshold 0.7 --top-k 15
        """
    )

    parser.add_argument('-i', '--input', dest='input_pdf', help='Input PDF file path', required=True)
    parser.add_argument('-o', '--output', help='Output TSV file path (default: <input_filename>_edges.tsv)', default=None, required=True)
    parser.add_argument('-n', '--ner-threshold', type=float, default=0.65, help='NER confidence threshold (default: 0.65)')
    parser.add_argument('-k', '--top-k', type=int, default=10, help='Top K parameter for retrieval (default: 10)')
    parser.add_argument('-v', '--verbose', action='store_true', help='Enable verbose output')

    args = parser.parse_args()
    
    # Validate input file
    if not os.path.exists(args.input_pdf):
        print(f"Error: Input file '{args.input_pdf}' does not exist.", file=sys.stderr)
        sys.exit(1)
    
    if not args.input_pdf.lower().endswith('.pdf'):
        print(f"Error: Input file must be a PDF file.", file=sys.stderr)
        sys.exit(1)
    
    # Generate output filename if not provided
    if args.output is None:
        base_name = os.path.splitext(os.path.basename(args.input_pdf))[0]
        args.output = f"{base_name}_edges.tsv"
    
    # Ensure output directory exists
    output_dir = os.path.dirname(args.output)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    if args.verbose:
        print(f"Processing PDF: {args.input_pdf}")
        print(f"Output file: {args.output}")
        print(f"NER threshold: {args.ner_threshold}")
        print(f"Top K: {args.top_k}")
        print("-" * 50)
    
    try:
        # Load environment variables
        load_dotenv()
        
        # Create EdgeList instance and process
        if args.verbose:
            print("Initializing EdgeList processor...")
        
        el = EdgeList(
            file=args.input_pdf,
            ner_threshold=args.ner_threshold,
            k=args.top_k
        )
        
        if args.verbose:
            print("Processing PDF and extracting entities...")
        
        edge_list = el.fit_transform()
        
        if args.verbose:
            print(f"Generated {len(edge_list)} edges")
        
        # Save the edge list
        el.save_edge_list(args.output)
        
        if args.verbose:
            print("Processing completed successfully!")
            
    except Exception as e:
        print(f"Error processing file: {e}", file=sys.stderr)
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
