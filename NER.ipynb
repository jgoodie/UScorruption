{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a083e649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from gliner import GLiNER\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.cleaners.core import (clean, \n",
    "                                        group_broken_paragraphs, \n",
    "                                        clean_extra_whitespace)\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "from ollama import chat\n",
    "from pydantic import BaseModel\n",
    "from typing import Literal, List, Optional, Dict, Any\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f91c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = clean(text)\n",
    "    text = group_broken_paragraphs(text)\n",
    "    text = clean_extra_whitespace(text)\n",
    "    return text\n",
    "\n",
    "def extract_sentences(elements):\n",
    "    all_text = []\n",
    "    for e in elements:\n",
    "        if e.text:\n",
    "            # Clean the text\n",
    "            text = clean_text(e.text)\n",
    "            all_text.append(text)\n",
    "    return all_text\n",
    "\n",
    "def get_connections(sentences, model, labels):\n",
    "    connections = []\n",
    "    for sent in sentences:\n",
    "        c = []\n",
    "        entities = model.predict_entities(sent, labels, threshold=0.65)\n",
    "        for entity in entities:\n",
    "            c.append((entity[\"text\"], entity[\"label\"]))\n",
    "        c = list(set(c))  # Remove duplicates\n",
    "        if len(c) > 1:\n",
    "            connections.append(c)\n",
    "    return connections\n",
    "\n",
    "def create_edge_list(connections, file_name=\"edge_list.csv\"):\n",
    "    edge_list = []\n",
    "    with open(file_name, 'w') as f:\n",
    "        f.write(\"Source,Target\\n\")\n",
    "        for c in connections:\n",
    "            for i in range(len(c)):\n",
    "                source = c[0][0]\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                edge_list.append((source, c[i][0]))\n",
    "                f.write(f\"{source},{c[i][0]}\\n\")\n",
    "    return edge_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0512e16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GLiNER with the base model\n",
    "# model = GLiNER.from_pretrained(\"urchade/gliner_medium-v2.1\")\n",
    "# model = GLiNER.from_pretrained(\"urchade/gliner_large-v2.1\")\n",
    "model = GLiNER.from_pretrained(\"numind/NuNER_Zero-span\")\n",
    "labels = [\"Person\", \"Company\", \"Organization\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5932a23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"TrumpEpstein/PDF/Former Models for Donald Trump’s Agency Say They Violated Immigration Rules and Worked Illegally.pdf\"\n",
    "file_name = \"TrumpEpstein/edge_lists/\" + file.split(\"/\")[-1]\n",
    "elements = partition_pdf(file)\n",
    "sentences = extract_sentences(elements)\n",
    "connections = get_connections(sentences, model, labels)\n",
    "edge_list = create_edge_list(connections, file_name=f\"{file_name}.csv\")\n",
    "text = \" \".join([s for s in sentences])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dd4349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update this to use NER entities from GLiNER \n",
    "# TODO: Example: ('Gianfranco Ferré', 'Person'), ('Jean Paul Gaultier', 'Person'), ('Vogue', 'Organization')\n",
    "def identify_person(name: str, entity_type: str, rag_chain, max_attempts: int = 2) -> str:\n",
    "    \"\"\"\n",
    "    Identify a person's full name using the RAG pipeline with structured output.\n",
    "    \"\"\"\n",
    "    queries = [\n",
    "        f\"What is the full name of {name}?\",\n",
    "        f\"Find all mentions of {name} in the text. What is their complete name?\",\n",
    "        f\"Who is {name}? Provide their full name.\"\n",
    "    ]\n",
    "    \n",
    "    for i, query_text in enumerate(queries[:max_attempts]):\n",
    "        try:\n",
    "            # Pass both name and entity_type as separate inputs\n",
    "            query = {\n",
    "                \"input\": name,\n",
    "                \"entity_type\": entity_type\n",
    "            }\n",
    "            response = rag_chain.invoke(query)\n",
    "            answer = response['answer'].strip()\n",
    "            # Post-process the answer for consistency\n",
    "            return format_name_response(name, entity_type, answer)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"Exception occurred:\", e)\n",
    "            if i == max_attempts - 1:\n",
    "                return f\"{name} (unknown)\"\n",
    "            continue\n",
    "    \n",
    "    return f\"{name} (unknown)\"\n",
    "\n",
    "def format_name_response(input_name: str, entity_type: str, response: str) -> str:\n",
    "    \"\"\"\n",
    "    Format the response to ensure consistent output format.\n",
    "    \"\"\"\n",
    "    response_lower = response.lower()\n",
    "    # Check for explicit indicators\n",
    "    if any(indicator in response_lower for indicator in ['pseudonym', 'fake name', 'alias', 'not real']):\n",
    "        # Extract the name part before adding (pseudonym)\n",
    "        clean_name = response.split('(')[0].strip() if '(' in response else input_name\n",
    "        return f\"{clean_name} (pseudonym)\"\n",
    "    \n",
    "    elif any(indicator in response_lower for indicator in [\"unclear\"]):\n",
    "        return f\"{input_name}, (unclear)\" # f\"{input_name}, (unknown)\"\n",
    "\n",
    "    elif any(indicator in response_lower for indicator in [\"don't know\", \"unknown\", \"not found\"]):\n",
    "        return f\"{input_name}, (unknown)\" # f\"{input_name}, (unknown)\"\n",
    "    \n",
    "    elif len(response.split()) >= 2 and not any(word in response_lower for word in ['unknown', 'pseudonym']) or any(word in response_lower for word in ['real', 'real name']):\n",
    "        # Looks like a full name\n",
    "        pattern = r'\\s*\\(real(?:\\s+name)?\\)'\n",
    "        cleaned = re.sub(pattern, '', response.strip())\n",
    "        # cleaned = re.sub(r'^[\\'\\\"]+|[\\'\\\"]+$', '', cleaned)\n",
    "        cleaned = cleaned.strip(\"'\\\"\")  # Remove any leading/trailing quotes\n",
    "        return cleaned\n",
    "\n",
    "    else:\n",
    "        print(f\"Unexpected response format: {response}\")\n",
    "        return f\"{input_name} ({entity_type})\"\n",
    "    \n",
    "# Enhanced validation function for GLiNER results\n",
    "def validate_ner_entities(connections, rag_chain):\n",
    "    \"\"\"\n",
    "    Validate and enhance GLiNER entity extractions using RAG.\n",
    "    \"\"\"\n",
    "    validated_connections = []\n",
    "    \n",
    "    for connection in connections:\n",
    "        validated_connection = []\n",
    "        \n",
    "        for entity_text, entity_label in connection:\n",
    "            if entity_label == \"Person\":\n",
    "                # Use RAG to get full name with entity type hint\n",
    "                full_name = identify_person(entity_text, entity_label, rag_chain)\n",
    "                validated_connection.append((full_name, entity_label))\n",
    "            else:\n",
    "                # For non-person entities, still pass the type but expect it back as-is\n",
    "                enhanced_name = identify_person(entity_text, entity_label, rag_chain)\n",
    "                validated_connection.append((enhanced_name, entity_label))\n",
    "        \n",
    "        validated_connections.append(validated_connection)\n",
    "    \n",
    "    return validated_connections\n",
    "    \n",
    "    return validated_connections\n",
    "\n",
    "def quality_check_validation(original_entities, validated_entities):\n",
    "    \"\"\"\n",
    "    Compare original GLiNER output with validated results.\n",
    "    \"\"\"\n",
    "    print(\"=== Validation Report ===\")\n",
    "    for i, (orig, val) in enumerate(zip(original_entities, validated_entities)):\n",
    "        print(f\"\\nConnection {i+1}:\")\n",
    "        for (orig_text, orig_label), (val_text, val_label) in zip(orig, val):\n",
    "            if orig_label == \"Person\" and orig_text != val_text:\n",
    "                print(f\"  Enhanced: {orig_text} → {val_text}\")\n",
    "            else:\n",
    "                print(f\"  Unchanged: {orig_text}\")\n",
    "\n",
    "def check_name(name):\n",
    "    context = \" \".join([r.page_content.strip(\".\").strip() for r in retriever.batch([name])[0]])\n",
    "    class Person(BaseModel):\n",
    "        name: str\n",
    "\n",
    "    response = chat(\n",
    "    messages=[\n",
    "        {\n",
    "        'role': 'user',\n",
    "        'content': f\"\"\"Tell me about {name}. Here is some context: {context}. \n",
    "                        Provide the full name if available. \n",
    "                        If the name is not a person, return the name as is.\n",
    "                        If the name is not a person (e.g., a company or organization), return the name as is without any additional labels.\"\"\",\n",
    "        }\n",
    "    ],\n",
    "    model='phi3.5', format=Person.model_json_schema(),\n",
    "    )\n",
    "\n",
    "    person = Person.model_validate_json(response.message.content)\n",
    "    return person\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5fb370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LLM to use with Ollama\n",
    "llm = OllamaLLM(model=\"mistral-nemo\", temperature=0)\n",
    "# llm = OllamaLLM(model=\"phi3.5\", temperature=0)\n",
    "\n",
    "# Initialize embeddings and vector store\n",
    "# Note: OllamaEmbeddings requires the Ollama server to be running with the specified model\n",
    "# Ensure you have the Ollama server running with the model \"nomic-embed-text\" or \"mxbai-embed-large\"\n",
    "# Note: \"mxbai-embed-large\" is a larger model and may take more time to process but provides better quality embeddings\n",
    "# embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "vectorstore = InMemoryVectorStore(embeddings)\n",
    "docs = []\n",
    "for i, s in enumerate(sentences):\n",
    "    docs.append(Document(id=str(i), page_content=s, metadata={\"source\": file_name}))\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "# Use smaller chunks for more precise retrieval\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,  # Smaller chunks for better precision\n",
    "    chunk_overlap=55,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \"]\n",
    ")\n",
    "# text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "all_texts = text_splitter.split_documents(docs)\n",
    "vs = vectorstore.from_documents(documents=all_texts, embedding=embeddings)\n",
    "# retriever = vs.as_retriever(k=6)\n",
    "# Increase retrieval count and add similarity threshold\n",
    "retriever = vs.as_retriever(search_kwargs={\"k\": 5, \"score_threshold\": 0.95}) # 0.3\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are a precise name identification assistant. Your task is to identify the full name of a person from the given context.\\n\"\n",
    "    \"The entity type hint is: {entity_type}\\n\"\n",
    "    \"Rules:\\n\"\n",
    "    \"1. If the entity type is 'Person' and you find the person's full name (first + last), return it exactly as written\\n\"\n",
    "    \"2. If the entity type is 'Person' and the name is explicitly mentioned as a pseudonym, fake name, or alias, return 'Name (pseudonym)'\\n\"\n",
    "    \"3. If the entity type is 'Person' and you cannot find the full name, return 'Name (unknown)'\\n\"\n",
    "    \"4. If the entity type is 'Organization' or 'Company', return the name as-is without any additional labels\\n\"\n",
    "    \"5. If the name is mentioned but unclear, or if there are multiple possibilities, return 'Name (unclear)'\\n\"\n",
    "    \"6. Only use information explicitly stated in the context\\n\"\n",
    "    \"7. Be concise - provide only the name and status\\n\"\n",
    "    \"8. Be factual. Do not invent new names. If you don't know the answer, respond with 'Name (unknown)'\\n\\n\"\n",
    "    \"Examples:\\n\"\n",
    "    \"- Input: 'Rachel' (Person) → Output: 'Rachel Blais' (if full name found)\\n\"\n",
    "    \"- Input: 'Kate' (Person) → Output: 'Kate (pseudonym)' (if mentioned as fake name)\\n\"\n",
    "    \"- Input: 'Vogue' (Organization) → Output: 'Vogue' (return as-is)\\n\\n\"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"Name: {input}\\nEntity Type: {entity_type}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "qa_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, qa_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4e7bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ('Vogue', 'Organization')\n",
    "# query = {\"input\": f\"Who is {name}? Give me their full name.\"}\n",
    "query = {\"input\": \"Vogue\", \"entity_type\": \"Organization\"}\n",
    "response = rag_chain.invoke(query)\n",
    "print(response['answer'])\n",
    "print(\"\\n\".join([r.page_content for r in response['context']]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b123682",
   "metadata": {},
   "outputs": [],
   "source": [
    "[r.page_content.strip(\".\").strip() for r in retriever.batch([\"Vogue (Organization)\"])[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c72f2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify_person(\"Trump\", rag_chain) # unclear\n",
    "# identify_person(\"Melania\", rag_chain) # unknown\n",
    "# res = identify_person(\"Vogue\", rag_chain) # unknown\n",
    "# print(res) \n",
    "# check_name(name=res)\n",
    "# Example usage\n",
    "# result = identify_person(\"Blais\", \"Person\", rag_chain)\n",
    "result = identify_person(\"Vogue\", \"Organization\", rag_chain)\n",
    "print(result) \n",
    "check_name(name=result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b815433",
   "metadata": {},
   "outputs": [],
   "source": [
    "connections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a3b363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_name(name=\"Gehi, (unknown)\")\n",
    "# check_name(name=\"Vogue, (unknown)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d93232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = \"Kate\"\n",
    "# # query = {\"input\": f\"Who is {name}? Give me their full name.\"}\n",
    "# query = {\"input\": name}\n",
    "# response = rag_chain.invoke(query)\n",
    "# print(response['answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5a1dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply validation to your connections\n",
    "# validated_connections = validate_ner_entities(connections, rag_chain)\n",
    "\n",
    "# # Test with specific names\n",
    "# test_names = [\"Kate\", \"Anna\", \"Blais\", \"Melania\"]\n",
    "# for name in test_names:\n",
    "#     result = identify_person(name, rag_chain)\n",
    "#     print(f\"{name} → {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfdc197",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Run quality check\n",
    "# quality_check_validation(connections, validated_connections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88cbf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify_person(\"kate\", rag_chain)\n",
    "# identify_person(\"gary\", rag_chain)\n",
    "# identify_person(\"Melania\", rag_chain)\n",
    "# identify_person(\"anna\", rag_chain)\n",
    "# identify_person(\"donald\", rag_chain)\n",
    "# identify_person(\"Lanzano\", rag_chain)\n",
    "# identify_person(\"Pierre Roussel\", rag_chain)\n",
    "# identify_person(\"Naresh\", rag_chain)\n",
    "# identify_person(\"Gehi\", rag_chain, max_attempts=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8a03a8",
   "metadata": {},
   "source": [
    "# Trying different NER models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6563ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9a168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = \"dslim/bert-base-NER\"\n",
    "m =\"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(m)\n",
    "model = AutoModelForTokenClassification.from_pretrained(m)\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = \"My name is Wolfgang and I live in Berlin\"\n",
    "# example = \"Hugging Face is based in New York City.\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "for e in ner_results:\n",
    "    print(f\"{e['word']}: {e['entity']} (score: {e['score']:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a16f840",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "entities = nlp(\"TRUMP\")\n",
    "\n",
    "# print(entities)\n",
    "for e in entities:\n",
    "    print(f\"{e['word']}: {e['entity']} (score: {e['score']:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa86c30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c45e4205",
   "metadata": {},
   "source": [
    "### Notes on RAG\n",
    "\n",
    "Example on how to use InMemoryVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7bd923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the LLM to use with Ollama\n",
    "# llm = OllamaLLM(model=\"mistral-nemo\", temperature=0)\n",
    "\n",
    "# # Initialize embeddings and vector store\n",
    "# # Note: OllamaEmbeddings requires the Ollama server to be running with the specified model\n",
    "# # Ensure you have the Ollama server running with the model \"nomic-embed-text\"\n",
    "# embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "# vectorstore = InMemoryVectorStore(embeddings)\n",
    "# docs = []\n",
    "# for i, s in enumerate(sentences):\n",
    "#     docs.append(Document(id=str(i), page_content=s, metadata={\"source\": file_name}))\n",
    "\n",
    "# # text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "# text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "# all_texts = text_splitter.split_documents(docs)\n",
    "# vs = vectorstore.from_documents(documents=all_texts, embedding=embeddings)\n",
    "# retriever = vs.as_retriever(k=6)\n",
    "# qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "\n",
    "# name = \"Blais\"\n",
    "# query = f\"Who is {name}? Give me their full name. Be concise. I just want first name and last name.\"\n",
    "# response = qa_chain.invoke(query)\n",
    "# print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332241b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RAG\n",
    "\n",
    "# pdf = \"How A.I. Assistants Could Supercharge Workplace Software _ Inc.com.pdf\"\n",
    "# docs = PyPDFLoader(pdf).load()\n",
    "\n",
    "# llm = OllamaLLM(model=\"phi3.5\", temperature=0)\n",
    "# embeddings = OllamaEmbeddings(model=\"all-minilm\")\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "# splits = text_splitter.split_documents(docs)\n",
    "# vectorstore = InMemoryVectorStore.from_documents(\n",
    "#     documents=splits, embedding=embeddings\n",
    "# )\n",
    "\n",
    "# retriever = vectorstore.as_retriever()\n",
    "\n",
    "# system_prompt = (\n",
    "#     \"You are an assistant for question-answering tasks. \"\n",
    "#     \"Use the following pieces of retrieved context to answer \"\n",
    "#     \"the question. If you don't know the answer, say that you \"\n",
    "#     \"don't know. Use three sentences maximum and keep the \"\n",
    "#     \"answer concise.\"\n",
    "#     \"\\n\\n\"\n",
    "#     \"{context}\"\n",
    "# )\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\", system_prompt),\n",
    "#         (\"human\", \"{input}\"),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "# rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "# excerpts = rag_chain.invoke({\"input\": \"Generate the top 5 key themes and phrases. Do not be overly verbose.\"})\n",
    "# print(excerpts['answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e8edde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the LLM to use with Ollama\n",
    "# # llm = OllamaLLM(model=\"mistral-nemo\", temperature=0)\n",
    "# llm = OllamaLLM(model=\"phi3.5\", temperature=0)\n",
    "\n",
    "# # Initialize embeddings and vector store\n",
    "# # Note: OllamaEmbeddings requires the Ollama server to be running with the specified model\n",
    "# # Ensure you have the Ollama server running with the model \"nomic-embed-text\" or \"mxbai-embed-large\"\n",
    "# # Note: \"mxbai-embed-large\" is a larger model and may take more time to process but provides better quality embeddings\n",
    "# # embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "# embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "# vectorstore = InMemoryVectorStore(embeddings)\n",
    "# docs = []\n",
    "# for i, s in enumerate(sentences):\n",
    "#     docs.append(Document(id=str(i), page_content=s, metadata={\"source\": file_name}))\n",
    "\n",
    "# # text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "# text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "# all_texts = text_splitter.split_documents(docs)\n",
    "# vs = vectorstore.from_documents(documents=all_texts, embedding=embeddings)\n",
    "# retriever = vs.as_retriever(k=6)\n",
    "\n",
    "# system_prompt = (\n",
    "#     \"You are an assistant for question-answering tasks. \"\n",
    "#     \"Specifically, your goal is to help identify people and their full names.\"\n",
    "#     \"If you don't know the person's full name, say that you don't know.\"\n",
    "#     \"If the name is a pseudonym, indicate that it is a pseudonym: Bob <PSEUDONYM>.\"\n",
    "#     \"Use the following pieces of retrieved context to answer the question.\"\n",
    "#     \"Only provide the full name  or name <PSEUDONYM> if you are sure about it, otherwise say you don't know.\"\n",
    "#     \"Be concise.\"\n",
    "#     \"\\n\\n\"\n",
    "#     \"{context}\"\n",
    "# )\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\", system_prompt),\n",
    "#         (\"human\", \"{input}\"),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "# qa_chain = create_stuff_documents_chain(llm, prompt)\n",
    "# rag_chain = create_retrieval_chain(retriever, qa_chain)\n",
    "\n",
    "# name = \"Kate\"\n",
    "# query = {\"input\": f\"Who is {name}? Give me their full name. If the name is a pseudonym, indicate that it is a pseudonym like this: Name <PSEUDONYM>. Be concise.\"}\n",
    "# response = rag_chain.invoke(query)\n",
    "# print(response['answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f12e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def identify_person(name: str, rag_chain, max_attempts: int = 2) -> str:\n",
    "#     \"\"\"\n",
    "#     Identify a person's full name using the RAG pipeline with structured output.\n",
    "#     \"\"\"\n",
    "#     queries = [\n",
    "#         f\"What is the full name of {name}? Is {name} a real name or a pseudonym?\",\n",
    "#         f\"Find all mentions of {name} in the text. What is their complete name?\",\n",
    "#         f\"Who is {name}? Provide their full name and indicate if it's a pseudonym or alias.\"\n",
    "#     ]\n",
    "    \n",
    "#     for i, query_text in enumerate(queries[:max_attempts]):\n",
    "#         try:\n",
    "#             query = {\"input\": query_text}\n",
    "#             response = rag_chain.invoke(query)\n",
    "#             answer = response['answer'].strip()\n",
    "            \n",
    "#             # Post-process the answer for consistency\n",
    "#             return format_name_response(name, answer)\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "#             if i == max_attempts - 1:  # Last attempt\n",
    "#                 return f\"{name} (unknown)\"\n",
    "#             continue\n",
    "#     print(\"LLM not queried, returning unknown\")\n",
    "#     return f\"{name} (unknown)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a39177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system_prompt = (\n",
    "#     \"You are a precise name identification assistant. Your task is to identify the full name of a person from the given context.\\n\"\n",
    "#     \"Rules:\\n\"\n",
    "#     \"1. If you find the person's full name (first + last), return it exactly as written\\n\"\n",
    "#     \"2. If the name is explicitly mentioned as a pseudonym, fake name, or alias, return 'Name (pseudonym)'\\n\"\n",
    "#     \"3. If you cannot find the full name, return 'Name (unknown)'\\n\"\n",
    "#     \"4. If the name is mentioned but unclear, or if there are multiple possibilities for example John Smith and Mary Smith, return 'Name (unclear)'\\n\"\n",
    "#     \"5. Only use information explicitly stated in the context\\n\"\n",
    "#     \"6. Be concise - provide only the name and status\\n\"\n",
    "#     \"7. Be factual. Do not invent new names. If you don't know the answer, respond with 'Name (unknown)'\\n\"\n",
    "#     \"8. If the name is not a person (e.g., a company or organization), return the name as is without any additional labels.\\n\\n\"\n",
    "#     \"Examples:\\n\"\n",
    "#     \"- Input: 'Rachel' → Output: 'Rachel Blais' (if full name found)\\n\"\n",
    "#     \"- Input: 'Kate' → Output: 'Kate (pseudonym)' (if mentioned as fake name or pseudonym)\\n\"\n",
    "#     \"- Input: 'Bob' → Output: 'Bob (unknown)' (if full name not found)\\n\\n\"\n",
    "#     \"Context: {context}\"\n",
    "# )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
